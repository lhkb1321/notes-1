
# Monitor Notes

## TODO
book: O'Reilly的Effective Monitoring and Alerting

## Tools
javamelody https://github.com/javamelody/javamelody/wiki
Is there a free and open source On-Prem alternative to NewRelic available for use in Java apps? https://www.reddit.com/r/java/comments/4ph65q/is_there_a_free_and_open_source_onprem/  
Glowroot Open source Java APM https://glowroot.org/  
metrics http://metrics.dropwizard.io/3.1.0/  
http://blog.takipi.com/java-performance-monitoring-5-open-source-tools-you-should-know/  

## Think
如果你正在开发的是Web系统，那么你不应当只把程序写完往服务器一扔就完事，而至少应当清楚一系列数据：
它每天要处理多少请求；这些请求来自何处，特点是怎样的；
系统每秒钟能处理多少请求，最大的并发量是怎样的，每个请求的处理时间是多少，会有怎样的波动……
如果这些数据你都了然于胸，再懂一点USE（Utility, Saturation, Error）分析方法，可以说你就已经具备了最基本的架构意识，不但可以找到现在系统的瓶颈，更能够未雨绸缪针对未来制定某些方案了。

1. 外部监控接口 定时访问以确定是否可连通: 博睿, catchpoint.com, dns port
2. 突发流量 API traffic 恶意请求如何发现:
2.1 实时分析日志?
2.2 接口内部实现记录
3. 缓存命中率记录并展现
4. 异常请求监控并及早丢弃

2）api可视化 你的app上架了，后端也布好了。然后呢，就天天看下载量。但后端好坏一问三不知。所以在设计api时，要提前回答以下问题：
1.此时此刻，有多少个app正在调用这些api，每分钟多少个？
2.他们（api）有快？
3.能否很简单地通过浏览器快速debug？
4.能否快速禁止单个用户？

4）api quota与perfcounter 对api进行quota限制，针对每个api每个人，都有限额。
这个限制的传统做法：
1.每天的上限调用次数。每天半夜清空，一天一个量累计。
2.每分钟的频度。这个防止有恶性的突发情况。
上述二者，缺一不可。
perfcounter用于对api的监控设计指标，如果一个api有异常，应当十分灵敏地得出结论，而不是误报连连。

REST API还应该有如下功能：
rate limiting：访问限制。防范 DDoS 攻击
metrics：服务器应该收集每个请求的访问时间，到达时间，处理时间，latency，便于了解API的性能和客户端的访问分布，以便更好地优化性能和应对突发请求。
docs：丰富的接口文档 - API的调用者需要详尽的文档来正确调用API，可以用swagger来实现。
作者：陈天
链接：https://zhuanlan.zhihu.com/p/20034107

过载保护
被攻击的问题
1.发短信攻击 做一些蜜罐的机制，当发现有异常，返回正常值，让攻击者觉得正常，其实服务端什么也没做，这是蜜罐机制。要真正解决这类问题，跟安全部一起做了试点，端上做这种是比较简单，认证比较容易。但是在 app 上不好做，通过 JavaScript 算，其实比较麻烦，并且容易别识破。另外还可以做一些人机识别机制
2.攻击者有两种攻击，一种拿着手机号，换不同的 code，这是一种攻击。另外一种攻击，拿固定的 code 换不同的手机号
过载保护 独立的访问控制层Argus
防控就是 Argus 系统，承载了过载保护，白名单、安全策略等等职责。它是独立的服务，所有的业务流量打过来，都需要通过它做过滤。

传统反爬虫手段
1、后台对访问进行统计， 如果单个IP访问超过阈值， 予以封锁。
这个虽然效果还不错， 但是其实有两个缺陷, 一个是非常容易误伤普通用户， 另一个就是， IP其实不值钱， 几十块钱甚至有可能买到几十万个IP。 所以总体来说是比较亏的。 不过针对三月份呢爬虫， 这点还是非常有用的。
2、后台对访问进行统计， 如果单个session访问超过阈值， 予以封锁。
这个看起来更高级了一些， 但是其实效果更差， 因为session完全不值钱， 重新申请一个就可以了。
3、后台对访问进行统计， 如果单个userAgent访问超过阈值， 予以封锁。
这个是大招， 类似于抗生素之类的， 效果出奇的好， 但是杀伤力过大， 误伤非常严重， 使用的时候要非常小心。 至今为止我们也就只短暂封杀过mac下的火狐。
4、以上的组合
组合起来能力变大， 误伤率下降， 在遇到低级爬虫的时候， 还是比较好用的。

## 构建一个比较完善的监控系统
http://caiguangguang.blog.51cto.com/1652935/1344826  

在工作中或多或少接触过部分监控工具的构建，开发和完善。结合自己的一些看法，简单地谈下一个完整的监控系统所包含的组件，欢迎大家补充。

从监控的层面来看，一个比较完整的监控系统应该包含如下的层次：

1.网络层面
主要包含各个机房间网络状况，机房内机器网络状况，通过开源的工具smokeping可以做到

2.主机层面
对于网络设备来说，包括cpu,内存，端口速率，流量等

对于服务器来说，从硬件到系统层面都要进行完善的监控，底层的比如raid卡状态，磁盘容量使用状态，inode使用状况，是否只读，是否有坏道，内存使用，网卡是否百M，网卡流量等。
再上面比如负载，cpu使用，磁盘io，操作系统参数，文件描述符，网络连接数，队列，系统日志等等，现在用的比较多的工具是zabbix，nagios和cacti。

3.应用层面可用性监控
包括进程的状态，端口的状况，存活的检查等，比如监控nginx，需要监控nginx的端口状态，进程是否存活，监控dns，需要监控端口，进程，还可以监控记录是否正常解析。
这些可以简单地通过zabbix实现。

4.应用层面性能监控
在应用层仅仅做可用性的监控是不够的，还要对应用的性能做监控，这就需要用户对应用的性能影响点有深入的了解。比如监控redis，需要监控redis的cpu使用情况，cache的效率等。
监控tomcat这种jvm容器，需要监控java heap,gc,java thread等相关信息。这类监控数据也可以集成搭配zabbix里面来做

5.服务的可用性
简单地来说就是业务正不正常，这个可以在业务上线的时候统一提供一个监控url来做检测，判断url的返回码或者是内容来判断服务是否正常。提供的检测url要做一些可用性方面的简单地判
断，比如如果应用用到了redis就要去操作一下redis。这一层的监控可以通过自己开发一个小的监控程序来实现（目前我们是通过pycurl+nagios+django+bootstrap来做的）。

6.服务的qos监控
服务的可用性监控有了，服务的质量状况也需要监控。
这个又包括服务器端的和面向用户的。
1）服务器端的数据一般是通过分析访问日志来实现，比如域名的nginx的响应时间分布，平均响应时间，服务的状态（2xx,3xx,4xx,5xx占的比例），平均body size大小状况，各个pool的调用状
况，具体到省市的qos等等。
服务器端的数据也分为实时的和离线的，实时的可以借助flume+hadoop+impala或者storm的流式计算框架来实现（各个域名的nginx日志），离线的数据可以通过hadoop + hive来实现（商业cdn qos数据）

2）面向用户的数据是通过在页面埋点来实现的，通过在页面中埋入js代码，可以在用户访问页面时，将所需要的数据通过url的形式记录在访问日志上，然后交由后端处理。

通过分析服务器端的和面向用户的qos数据相结合，可以很清楚的了解到网站的qos状况。

7.安全方面的监控
又包括主机层和服务层，比如webshell检测，主机入侵检测，后门检测，文件校验等等。

8.最后就是业务上的监控了，这个没怎么接触过，就不写了

监控的数据有了之后就是报警的问题了，这里我们倾向于：

1.通过一个中心化的事件处理平台来处理报警信息，集成cmdb的信息，把报警信息分组，定义级别并发送至相关责任人处理

2.报警邮件要包含足够多的信息，比如具体到一个域名的http code不正常，需要在报警邮件中有breakdown到http code,server，url等的相关信息，这样，报警处理人就可以快速的定位到问题。

最后附一个之前画的一个基于zabbix的一个监控的流程图：
1）用户提交监控配置和脚本至puppet server，并关联cmdb的业务和puppet的module,由puppet 实现zabbix agent的部署，更新，reload。
2）用户通过zabbix api一键式添加模板，服务器根据主机名，运行应用自动link到对应的模板，主机组等。
3）使用zabbix的server-proxy结构，zabbix agent使用passive的模式，proxy使用active的模式，由proxy负责收集数据，sync到server。
4）server的db做ms高可用，并将slave中zabbix的历史数据（history*表，trends表）通过sqoop导入hadoop集群，load至hive中归档。
5）用户可以基于hive中的历史数据做性能分析和容量规划报表。
6）zabbix server负责报警产生，并通过消息机制发送至事件处理中心，事件处理中心关联cmdb，发送至对应业务的对应负责人。

## 电商峰值监控经验谈
http://www.csdn.net/article/2014-11-11/2822582  
### 监控内容
1. 最底层是对服务器资源的监控，包括硬盘可用空间、CPU使用率、内存占用、I/O、网络流量等指标
2. 第二层是对网络链路层面的监控，其中包括内部网络状况的监控，例如集群之间的网络连通性、路由情况，还包括外部用户的网络状态监控，例如DNS、CDN服务质量等指标
3. 第三层是应用层面的监控，包括Web应用容器、数据库、NoSQL、手机App的指标，例如Cache命中率、JVM状态是否正常、每秒的请求量（QPS）、请求响应时间、请求状态、请求队列长度、数据库响应时间、慢SQL性能、Memcache、Redis服务状态、App打开率、App交互性能、App崩溃等指标
4. 最顶层是业务层面的监控，包括关键业务的处理能力和业务逻辑的流畅程度等指标，例如单位时间的订单数量、新客户注册数量、客户投诉数量、关键业务的队列排队数量、用户查询购买某一商品的流畅度等

### 监控方法 构建监控平台通常有以下3种方式。
1. 利用开源软件和平台结合shell脚本搭建监控平台。
2. 自行研发监控工具和监控平台。
3. 购买第三方的监控服务。

#### 服务器资源的监控方法
除了使用操作系统自带的基础命令（如top、 free)采集数据以外，阿里、京东等资金雄厚且技术能力强的知名电商公司，开发并开源了一系列监控工具，例如Tsar是阿里巴巴开源的采集工具，主要用来收集服务器的系统信息（如CPU、I/O、Mem和TCP等）和应用层面的数据（如Squid、HAProxy、Nginx等），用户也可以根据需求编写自己的采集模块，集成到Tsar中即可使用，产生的数据输出到远程数据库或者集成到Nagios或Cacti中即可显示。

#### 网络链路层面的监控方法
监控内部网络的状态采取与监控服务器资源类似的方式，使用脚本调用操作系统自带的命令（Ping和Tracert等）或其他开源工具进行数据采集，数据集成到Nagios或Cacti中展示。

有两种监控外部网络状态的方法可以得到用户使用网站的体验数据。一种方法是自行开发并运营监控网络，此类方法适用于资金雄厚的公司，例如阿里开发了Alibench，并在全国征集志愿者安装这个工具，探测网站访问是否正常并将探测到的数据上传到服务器进行汇总分析。另一种方法是购买第三方服务，例如听云Network服务，以付费的方式招募会员，并在全球安装监测节点。这两种方法的技术原理相似，都是通过在全国或全球的终端用户电脑上部署探测工具，探测工具内嵌IE、Chrome、Firefox这些主流浏览器内核，获取访问网站过程中的DNS解析、TCP建立连接、页面DOM解析和页面渲染等过程的详细数据。通过查看分类汇总的数据分析哪些用户访问网站时出现了错误或性能瓶颈，哪些CDN提供的加速效果更优秀，从哪些角度可以进一步优化网站。

#### 应用层面的监控方法
常规针对应用层面的监控方法是，利用脚本对应用服务器产生的日志进行统计分析，或利用应用服务器自带的统计分析命令和接口进行数据汇总。

在Web应用容器层面通过HTTP请求访问日志可以统计请求量（QPS）、请求响应时间、错误等信息，通过JVM命令查询JVM状态；在数据库层面，开启慢SQL查询日志，使用数据库相关命令统计慢SQL语句。

第三方APM服务厂商的应用性能管理服务，例如听云Server服务，采取自动跟踪Web应用容器内HTTP请求的出入口和应用代码函数方式，不仅可以采集应用运行时的QPS、请求响应时间、错误信息、SQL调用等信息，还可以采集到常规方法无法获取的代码级别的性能信息，进而分析应用的瓶颈所在。

对于近几年兴起的移动App而言，监控App应用需要研发人员嵌入第三方的SDK代码。SDK代码在App运行时统计用户的交互行为、打开情况和崩溃信息，在一定时间段内汇总信息，提交到监控平台。

#### 业务层面的监控方法
对关键业务处理能力的监控，通常采取研发人员预留API接口，使用脚本调用API接口获取关键业务处理数据，并将数据集成到Nagios或Cacti中展示的方法。

业务逻辑流畅程度的监控需要购买第三方的业务流程监控功能，此类工具需要熟悉业务的人员录制业务流程的脚本，并上传到第三方监控平台。平台将流程脚本部署到第三方监控节点上，并自动回放流程脚本模拟用户对流程的操作，最终平台会采集到每一步骤的性能和错误信息。


## [Mercury:唯品会全链路应用监控系统解决方案详解](https://mp.weixin.qq.com/s?__biz=MzAwMDU1MTE1OQ==&idx=1&mid=2653547643&sn=c06dc9b0f59e8ae3d2f9feb734da4459#wechat_redirect)
姚捷 高可用架构  
基于 nginx access log 的监控平台  
基于客户端探针上报应用调用链相关日志:客户端会构建一个异步日志队列  
基于流式计算和大数据存储和检索技术  
提供实时和准实时告警,并快速定位根源问题  

系统层监控我们主要还是 Zabbix  
业务层我们有一个产品叫 Telescope，所有的业务监控，包括 PV/UV、定单量、支付成功/失败率等重要业务数据  
在应用层，现在 ELK 用得比较多，另外一个产品 SKYHAWK，它其实是 APP 应用端的监控平台  
全链路监控技术栈，我们的体系有用了很多的技术，Spark 、 Open  TSDB  、HBase  、Elastic Search，Kafka，Flume，Python  


## 分布式调用跟踪系统的设计和应用学习
邴越 [链接](https://yq.aliyun.com/articles/58408)

随着服务的拆分，系统的模块变得越来越多，不同的模块可能由不同的团队维护，一个请求可能会涉及到几十个服务的协同处理， 牵扯到多个团队的业务系统，那么如何快速准确的定位到线上故障？  
同时，缺乏一个自上而下全局的调用id，如何有效的进行相关的数据分析工作？  
比较成熟的解决方案是 **通过调用链的方式，把一次请求调用过程完整的串联起来，这样就实现了对请求调用路径的监控。**
### 调用跟踪系统的业务场景
* 故障快速定位
  通过调用链跟踪，一次请求的逻辑轨迹可以用完整清晰的展示出来。  
  开发中可以在业务日志中添加调用链ID，可以通过调用链结合业务日志快速定位错误信息。
* 各个调用环节的性能分析
  在调用链的各个环节分别添加调用时延，可以分析系统的性能瓶颈，进行针对性的优化。
* 各个调用环节的可用性，持久层依赖等
  通过分析各个环节的平均时延，QPS等信息，可以找到系统的薄弱环节，对一些模块做调整，如数据冗余等。
* 数据分析等
  调用链是一条完整的业务日志，可以得到用户的行为路径，汇总分析应用在很多业务场景。

### 分布式调用跟踪系统的设计
#### 目标
* 低侵入性，应用透明
* 低损耗
* 大范围部署，扩展性

#### 埋点和生成日志
通常包含：
1. TraceId、RPCId、调用的开始时间，调用类型，协议类型，调用方ip和端口，请求的服务名等信息；
2. 调用耗时，调用结果，异常信息，消息报文等；
3. 预留可扩展字段，为下一步扩展做准备；

#### 抓取和存储日志
一般来说，会使用离线+实时的方式去存储日志，主要是分布式日志采集的方式。典型的解决方案如Flume结合Kafka等MQ。
#### 分析和统计调用链数据
一条调用链的日志散落在调用经过的各个服务器上，首先需要按 TraceId 汇总日志，然后按照RpcId 对调用链进行顺序整理。  
调用链数据不要求百分之百准确，可以允许中间的部分日志丢失
#### 计算和展示
汇总得到各个应用节点的调用链日志后，可以针对性的对各个业务线进行分析。  
需要对具体日志进行整理，进一步储存在HBase或者关系型数据库中，可以进行可视化的查询。

### 调用跟踪系统的选型 Distributed Systems Tracing
大的互联网公司都有自己的分布式跟踪系统，比如Google的Dapper，Twitter的zipkin，淘宝的鹰眼，新浪的Watchman，京东的Hydra等

#### Google的Drapper
[Dapper, a Large-Scale Distributed Systems Tracing Infrastructure](http://research.google.com/pubs/pub36356.html)
[简要总结](http://duanple.blog.163.com/blog/static/70971767201329113141336/)
[完整译文](https://bigbully.github.io/Dapper-translation/)
Dapper是Google生产环境下的分布式跟踪系统，Dapper有三个设计目标：

* 低消耗：跟踪系统对在线服务的影响应该做到足够小。
* 应用级的透明：对于应用的程序员来说，是不需要知道有跟踪系统这回事的。如果一个跟踪系统想生效，就必须需要依赖应用的开发者主动配合，那么这个跟踪系统显然是侵入性太强的。
* 延展性：Google至少在未来几年的服务和集群的规模，监控系统都应该能完全把控住。

Drapper的日志格式：  
dapper用span来表示一个服务调用开始和结束的时间，也就是时间区间。
dapper记录了span的名称以及每个span的ID和父ID，如果一个span没有父ID被称之为root span。所有的span都挂在一个特定的trace上，共用一个traceID，这些ID用全局64位整数标示。

## 再谈系统 Monitoring 和 Alerting
朱赟 [嘀嗒嘀嗒] (https://mp.weixin.qq.com/s?__biz=MzA4ODgwNjk1MQ==&mid=2653788488&idx=1&sn=284ed9cf28461cee25ce4c9ddfa2e271&chksm=8bfdba00bc8a33160598fb3d9f06ec8785e736f8a132bc57ab987aa77bafbf0b89a5d77b1d10&mpshare=1&scene=23&srcid=1122rhD6YOujJreDU2ZXKKNN#rd )
1. Syslog 或 Kibana Log：也就是系统的日志.  日志系统的架构经过多年的演变，现在比较流行的一种应该是 ELK (ElasticSearch-LogStash-Kibana) 架构了
2. Hive 或者 DB Persisted Event：可以做比较详尽的 Event Tracking。这个根据不同的使用场景可以做的特别 Powerful。但是需要搭建比较完善的 Event Pipeline 和 Processing System。量大以后 Scalability 的处理也很重要。对工程师的要求比较高，尤其是在 Infrastructure 方面
3. Datadog：可以记录一些系统的 Statistic。打个比方。对一个 Charge 函数。可以入口的时候设一个计数器，出口的时候为成功的 Charge 和失败的 Charge 各设一个计数器。这样就可以用图表追踪 Charge 成功的 Rate。并且可以进一步设置一个 Alert，当这个 Rate 低于 99.9% 的时候就 Fire Alarm。Datadog 还可以做一些很细致的统计、画图和警报。很多公司都有大屏幕用 Datadog 实时 Monitor 系统的核心 Metrics。
4. Sentry 和 NewRelic：主要是实时 Track 系统的 Error 和异常。对一些 API 或者 Client、Server Error 进行 Aggregation。很多公司 Oncall 必不可少的监测手段。
5. 基于机器学习或者数学模型建立的 Alert 系统：比如之前《公司里的 Data Scientist（数据科学家）》一文中提过的我们建立的一个异常监测系统就属于这一类。
6. 基于 DB Trigger 的 Auditing Trail：记录所有的 DB 改动，可以还原所有数据库写操作的历史。
